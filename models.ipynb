{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d49d980-1a05-4a74-8fce-aa3566839b8e",
   "metadata": {},
   "source": [
    "!pip install xgboost\n",
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "818ff6a9-b123-474c-ad75-c5c01c83e0f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8e7c795-574b-4cbe-a19b-86f40a1954f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import   pandas                   as        pd\n",
    "import   numpy                    as        np\n",
    "import   matplotlib.pyplot        as        plt\n",
    "import   sklearn.metrics          as        metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a6293e4-3229-4a72-b2a4-ee7ca1c29ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from    sklearn.svm               import    SVC\n",
    "from    sklearn.ensemble          import    RandomForestClassifier, AdaBoostClassifier \n",
    "from    sklearn.model_selection   import    StratifiedKFold\n",
    "from    sklearn.model_selection   import    StratifiedGroupKFold\n",
    "from    sklearn.model_selection   import    RandomizedSearchCV\n",
    "from    sklearn.linear_model      import    LogisticRegression\n",
    "from    sklearn.tree              import    DecisionTreeClassifier\n",
    "from    sklearn.neighbors         import    KNeighborsClassifier\n",
    "from    sklearn.naive_bayes       import    GaussianNB   \n",
    "from    xgboost                   import    XGBClassifier  \n",
    "from    sklearn.metrics           import    make_scorer, precision_score, recall_score, f1_score\n",
    "from    sklearn.metrics           import    confusion_matrix, classification_report\n",
    "from    sklearn.model_selection   import    train_test_split,cross_val_score,cross_val_predict \n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8736e11a-dae3-4f33-bb4a-3e69699269e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_performance(name, model, x_train, y_train, x_test, y_test):\n",
    "    from sklearn.metrics import recall_score, precision_score,roc_auc_score,f1_score  \n",
    "    # Predict the response for training dataset\n",
    "    classifier         =  model.fit(x_train, y_train)\n",
    "    # print the model parameters\n",
    "    print(\"Model parameters: {}\".format(model.get_params()))\n",
    "    predicted_train    =  classifier.predict(x_train)\n",
    "    matrix             =  confusion_matrix(y_train, predicted_train)\n",
    "    ###\n",
    "    train_auroc        =  roc_auc_score(y_train, predicted_train)\n",
    "    train_recall       =  recall_score(y_train, predicted_train)\n",
    "    train_precision    =  precision_score(y_train, predicted_train)\n",
    "    train_f1score      =  f1_score(y_train, predicted_train, average = 'weighted')\n",
    "    ###\n",
    "    print(\"\\nTraining Data\")\n",
    "    print(matrix)\n",
    "    draw_cm(y_train, predicted_train)\n",
    "    measures_train      = classification_report(y_train, predicted_train) \n",
    "    print(\"\\nTraining Data\")\n",
    "    print(measures_train) \n",
    "    draw_roc(y_train, predicted_train)\n",
    "    # Predict the response for testing dataset\n",
    "    predicted_test     =  classifier.predict(x_test)\n",
    "    matrix1            =  confusion_matrix(y_test, predicted_test)\n",
    "    ### \n",
    "    test_auroc         =  roc_auc_score(y_test, predicted_test)\n",
    "    test_recall        =  recall_score(y_test, predicted_test)\n",
    "    test_precision     =  precision_score(y_test, predicted_test)\n",
    "    test_f1score       =  f1_score(y_test, predicted_test, average = 'weighted')\n",
    "    ###    \n",
    "    print(\"\\nTest  Data\")\n",
    "    print(matrix1)\n",
    "    draw_cm(y_test, predicted_test)\n",
    "    measures_test     = classification_report(y_test, predicted_test) \n",
    "    print(\"\\nTest  Data\")\n",
    "    print(measures_test) \n",
    "    draw_roc(y_test, predicted_test) \n",
    "    df_metrics    =  pd.DataFrame({'Model' : name, 'Recall Training data' : train_recall, 'Recall Test data' : test_recall,\\\n",
    "                              'F1 Weighted Training data' : train_f1score, 'F1 Weighted Test data' : test_f1score,\n",
    "                              'AUROC Training data' : train_auroc, 'AUROC Test data' : test_auroc,\n",
    "                              'Precision Training data' : train_precision, 'Precision Test data' : test_precision},\\\n",
    "                              index = [0]) \n",
    "    return df_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9127c0ed-6f42-452b-bfe4-eee3f0af60d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_cm( actual, predicted ):\n",
    "    \n",
    "    import matplotlib.pyplot as   plt\n",
    "    import sklearn.metrics   as   metrics\n",
    "    import seaborn           as   sns\n",
    "    \n",
    "    cm = metrics.confusion_matrix( actual, predicted)\n",
    "    sns.heatmap(cm, annot=True, fmt='.2f', xticklabels = [\"Yes\", \"No\"] , yticklabels = [\"Yes\", \"No\"] )\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fcc2e068-f5f8-46a4-a051-3ccc2084bcf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_roc( actual, probs ):\n",
    "    \n",
    "    fpr, tpr, thresholds = metrics.roc_curve( actual, probs,\n",
    "    drop_intermediate = False )\n",
    "    auc_score = metrics.roc_auc_score( actual, probs )\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.plot( fpr, tpr, label='ROC curve (area = %0.2f)' % auc_score )\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate or [1 - True Negative Rate]')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver operating characteristic curve')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "    return fpr, tpr, thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7fa4e2df-dd92-4194-8da5-2453acb57d39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: dictdiffer in c:\\users\\chitralekha\\anaconda3\\lib\\site-packages (0.9.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install dictdiffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4c0d0620-3178-4ee6-9030-602f036ae9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dictdiffer import diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "93ef92f5-6176-453d-8fca-6e28686e7aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_hyper_df       =    pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "13e54754-1447-4e5e-b6c3-24da83289ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time   =  datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b5cc9c90-0b3b-4558-b6d4-d59820f01181",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "We need to know what changed after getting the best parameters by performing hyper parameter tuning,\n",
    "'''\n",
    "def  what_changed(model_name, first_dict, second_dict):\n",
    "### First dict \n",
    "       first_dictKeys = list(first_dict.keys())\n",
    "       first_dictKeys.sort()\n",
    "       first_sorted_dict = {i: first_dict[i] for i in first_dictKeys}\n",
    "### Second dict \n",
    "       second_dictKeys = list(second_dict.keys())\n",
    "       second_dictKeys.sort()\n",
    "       second_sorted_dict = {i: second_dict[i] for i in second_dictKeys}\n",
    "\n",
    "       result = diff(first_sorted_dict, second_sorted_dict)\n",
    "       print(\"Model name {}\".format(model_name))\n",
    "       return list(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d18570d2-5642-455a-bea4-cd3ecd1cffbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def  metrics_graph(df, model_names):\n",
    "     print(df.columns)\n",
    "     scores_req   =   ['roc_auc', 'precision', 'recall', 'F1 Weighted']\n",
    "     for       scoring in   scores_req :\n",
    "               df1      =  df.loc[df['Score'] == scoring, ['Name', 'Results']]\n",
    "               df1.plot.box(column = \"Results\", by = \"Name\", figsize=(8, 6), grid=False, rot=90, fontsize = 15)\n",
    "               txt      =  \"Model performance using \" + scoring\n",
    "               plt.title(txt)\n",
    "               plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "434aab1f-0330-48cc-a902-375e4cf7b02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_default(model, X, y):\n",
    "    model.fit(X, np.ravel(y,order=\"c\"))\n",
    "    print(model.get_params())\n",
    "    return model.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "463a4a2f-4150-4ac4-bd38-a44d6b516ecb",
   "metadata": {},
   "source": [
    "#### Define dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9ea0e152-f8aa-440b-9d1a-06207cfc39ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          roc_auc  precision  recall  F1 Weighted\n",
      "Model                                            \n",
      "LR              0          0       0            0\n",
      "KNN             0          0       0            0\n",
      "CART            0          0       0            0\n",
      "NB              0          0       0            0\n",
      "RF              0          0       0            0\n",
      "XGBoost         0          0       0            0\n",
      "AdaBoost        0          0       0            0\n"
     ]
    }
   ],
   "source": [
    "df1_0    =   pd.DataFrame({'Model' : 'LR', 'roc_auc' : 0, 'precision' : 0, 'recall' : 0, 'F1 Weighted' :0}, index = [0])\n",
    "df1_0.reset_index()\n",
    "dict = {'Model' : 'KNN', 'roc_auc' : 0, 'precision' : 0, 'recall' : 0, 'F1 Weighted' :0}\n",
    "df12 = pd.DataFrame(dict, index = [0])\n",
    "#\n",
    "dict = {'Model' : 'CART', 'roc_auc' : 0, 'precision' : 0, 'recall' : 0, 'F1 Weighted' :0}\n",
    "df13 = pd.DataFrame(dict, index = [0])\n",
    "#\n",
    "dict = {'Model' : 'NB', 'roc_auc' : 0, 'precision' : 0, 'recall' : 0, 'F1 Weighted' :0}\n",
    "df14  = pd.DataFrame(dict, index = [0])\n",
    "#\n",
    "dict = {'Model' : 'RF', 'roc_auc' : 0, 'precision' : 0, 'recall' : 0, 'F1 Weighted' :0}\n",
    "df15  = pd.DataFrame(dict, index = [0])\n",
    "#\n",
    "dict = {'Model' : 'XGBoost', 'roc_auc' : 0, 'precision' : 0, 'recall' : 0, 'F1 Weighted' :0}\n",
    "df16  = pd.DataFrame(dict, index = [0])\n",
    "#\n",
    "dict = {'Model' : 'AdaBoost', 'roc_auc' : 0, 'precision' : 0, 'recall' : 0, 'F1 Weighted' :0}\n",
    "df17  = pd.DataFrame(dict, index = [0])\n",
    "##\n",
    "df17_ = pd.concat([df1_0, df12, df13, df14, df15, df16, df17], ignore_index = True)\n",
    "#\n",
    "\n",
    "df_measures_untuned  =  df17_.copy()\n",
    "df_measures_untuned.set_index(['Model'], inplace = True)\n",
    "print(df_measures_untuned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7c4c5845-a8b5-4796-a975-687c3f7d2329",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          roc_auc  precision  recall  F1 Weighted\n",
      "Model                                            \n",
      "LR              0          0       0            0\n",
      "KNN             0          0       0            0\n",
      "CART            0          0       0            0\n",
      "NB              0          0       0            0\n",
      "RF              0          0       0            0\n",
      "XGBoost         0          0       0            0\n",
      "AdaBoost        0          0       0            0\n"
     ]
    }
   ],
   "source": [
    "df_0    =   pd.DataFrame({'Model' : 'LR', 'roc_auc' : 0, 'precision' : 0, 'recall' : 0, 'F1 Weighted' :0}, index = [0])\n",
    "df_0.reset_index()\n",
    "dict = {'Model' : 'KNN', 'roc_auc' : 0, 'precision' : 0, 'recall' : 0, 'F1 Weighted' :0}\n",
    "df2 = pd.DataFrame(dict, index = [0])\n",
    "#\n",
    "dict = {'Model' : 'CART', 'roc_auc' : 0, 'precision' : 0, 'recall' : 0, 'F1 Weighted' :0}\n",
    "df3 = pd.DataFrame(dict, index = [0])\n",
    "#\n",
    "dict = {'Model' : 'NB', 'roc_auc' : 0, 'precision' : 0, 'recall' : 0, 'F1 Weighted' :0}\n",
    "df4  = pd.DataFrame(dict, index = [0])\n",
    "#\n",
    "dict = {'Model' : 'RF', 'roc_auc' : 0, 'precision' : 0, 'recall' : 0, 'F1 Weighted' :0}\n",
    "df5  = pd.DataFrame(dict, index = [0])\n",
    "#\n",
    "dict = {'Model' : 'XGBoost', 'roc_auc' : 0, 'precision' : 0, 'recall' : 0, 'F1 Weighted' :0}\n",
    "df6  = pd.DataFrame(dict, index = [0])\n",
    "#\n",
    "dict = {'Model' : 'AdaBoost', 'roc_auc' : 0, 'precision' : 0, 'recall' : 0, 'F1 Weighted' :0}\n",
    "df7  = pd.DataFrame(dict, index = [0])\n",
    "##\n",
    "df7_ = pd.concat([df_0, df2, df3, df4, df5, df6, df7], ignore_index = True)\n",
    "#\n",
    "\n",
    "df_measures_tuned  =  df7_.copy()\n",
    "df_measures_tuned.set_index(['Model'], inplace = True)\n",
    "print(df_measures_tuned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d336d9df-c77d-490a-b2e4-34ad46d0aaec",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names  =  [\"LR\",\"KNN\",\"CART\",\"NB\",\"RF\",\"XGBoost\",\"AdaBoost\"]\n",
    "models       =  [LogisticRegression(max_iter = 3000),\\\n",
    "                 KNeighborsClassifier(),\\\n",
    "                 DecisionTreeClassifier(),\\\n",
    "                 GaussianNB(),\\\n",
    "                 RandomForestClassifier(),\\\n",
    "                 XGBClassifier(),\\\n",
    "                 AdaBoostClassifier()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8fccbc11-ce6c-474d-9fef-bc8d1508242b",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\chitralekha\\\\Desktop\\\\Great Learning\\\\DSE-FT-CHN-MAY24-G5-Final_Report\\\\processed_data_x2024-12-03 09-53-44.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m X_            \u001b[38;5;241m=\u001b[39m   pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mchitralekha\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mDesktop\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mGreat Learning\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mDSE-FT-CHN-MAY24-G5-Final_Report\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mprocessed_data_x2024-12-03 09-53-44.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      2\u001b[0m y             \u001b[38;5;241m=\u001b[39m   pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mchitralekha\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mDesktop\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mGreat Learning\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mDSE-FT-CHN-MAY24-G5-Final_Report\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mprocessed_data_y2024-12-03 09-53-44.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[0;32m   1881\u001b[0m     f,\n\u001b[0;32m   1882\u001b[0m     mode,\n\u001b[0;32m   1883\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1884\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1885\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[0;32m   1886\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[0;32m   1887\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1888\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1889\u001b[0m )\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    874\u001b[0m             handle,\n\u001b[0;32m    875\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m    876\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[0;32m    877\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[0;32m    878\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    879\u001b[0m         )\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\chitralekha\\\\Desktop\\\\Great Learning\\\\DSE-FT-CHN-MAY24-G5-Final_Report\\\\processed_data_x2024-12-03 09-53-44.csv'"
     ]
    }
   ],
   "source": [
    "X_            =   pd.read_csv(r'C:\\Users\\chitralekha\\Desktop\\Great Learning\\DSE-FT-CHN-MAY24-G5-Final_Report\\processed_data_x2024-12-03 09-53-44.csv')\n",
    "y             =   pd.read_csv(r'C:\\Users\\chitralekha\\Desktop\\Great Learning\\DSE-FT-CHN-MAY24-G5-Final_Report\\processed_data_y2024-12-03 09-53-44.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09eb9c51-f9ff-4aa0-bd1f-d23df07637f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_, y, test_size = 0.2, shuffle = True, stratify = y, random_state = 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec5039c-f406-4087-bc67-19238b7b2dc1",
   "metadata": {},
   "source": [
    "### A)  Before SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85270574-04ac-4d80-9970-ed8cff2c5263",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df    =  pd.DataFrame()\n",
    "for   i    in   range(len(model_names)):\n",
    "      model_x            =   model_names[i]\n",
    "      print(\"i = %d Model = %s\" %(i,model_x))\n",
    "      classifier         =   models[i] \n",
    "      mp_df         =  evaluate_model_performance(model_x, classifier, X_train, y_train, X_test, y_test)\n",
    "      metrics_df    =  pd.concat([metrics_df, mp_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da938584-d3b6-4ada-972e-210fbbcede93",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df.sort_values(by=['Recall Test data'], ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff0675f-86f2-4ca9-870c-d48d05564912",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_elapsed = datetime.now() - start_time\n",
    "print('\\nExecution Time for evaluating the performance of 7 models on Raw data not treated for data imbalance')\n",
    "print('Time elapsed (hh:mm:ss.ms) {}'.format(time_elapsed))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ec80a2-97f5-43f8-a875-977dc218b3ba",
   "metadata": {},
   "source": [
    "### Synthetic Minority Oversampling TEchnique (SMOTE)\n",
    "\n",
    "A problem with imbalanced classification is that there are too few examples of the minority class for a model to effectively learn the decision boundary.\n",
    "\n",
    "One way to solve this problem is to **oversample** the examples in the minority class. This can be achieved by simply duplicating examples from the minority class in the training dataset prior to fitting a model. This can balance the class distribution but does not provide any additional information to the model.\n",
    "\n",
    "An improvement on duplicating examples from the minority class is to synthesize new examples from the minority class. This is a type of data augmentation for tabular data and can be very effective.\n",
    "\n",
    "Perhaps the most widely used approach to synthesizing new examples is called the Synthetic Minority Oversampling TEchnique, or SMOTE for short. This technique was described by Nitesh Chawla, et al. in their 2002 paper named for the technique titled “SMOTE: Synthetic Minority Over-sampling Technique.”\n",
    "\n",
    "SMOTE works by selecting examples that are close in the feature space, drawing a line between the examples in the feature space and drawing a new sample at a point along that line.\n",
    "\n",
    "Specifically, a random example from the minority class is first chosen. Then k of the nearest neighbors for that example are found (typically k=5). A randomly selected neighbor is chosen and a synthetic example is created at a randomly selected point between the two examples in feature space.\n",
    "\n",
    "**Undersampling** is a technique to balance uneven datasets by keeping all of the data in the minority class and decreasing the size of the majority class. It is one of several techniques data scientists can use to extract more accurate information from originally imbalanced datasets.\n",
    "\n",
    "Undersampling can result in the loss of relevant information by removing valuable and significant patterns.\n",
    "Undersampling is appropriate when there is plenty of data for an accurate analysis. The data scientist uses all of the rare events but reduces the number of abundant events to create two equally sized classes.\n",
    "\n",
    "We have **4062 (2.25%) observations for the minority class** and 176457 (97.75%) observations for the majority class. \n",
    "So, it was decided to go for **Oversampling** method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "437f6aff-afea-4cb7-9719-57037b0bf218",
   "metadata": {},
   "source": [
    "### b) After SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9814e2c7-a416-4787-adad-5a88ca45351c",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time   =  datetime.now()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db4316a-59e5-4be8-9d72-9927bbea0470",
   "metadata": {},
   "source": [
    "For appling SMOTE, we have splitted the data into training and test datasets in the ratio, 80%: 20% and applied SMOTE only on Training data.\n",
    "\n",
    "As we need to apply SMOTE only on Training data, we have not applied K Fold cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "620b4811-6af2-4157-aee5-d4aaab47b07c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# transform the dataset\n",
    "oversample = SMOTE()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948daac3-cc50-4a46-8ab4-c31e86682321",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_SMOTE, y_train_SMOTE = oversample.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07388e08-079e-4f9f-a67c-1bcb692ddf8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "SMOTE_metrics_df =  pd.DataFrame()\n",
    "for   i    in   range(len(model_names)):\n",
    "      model_x            =   model_names[i]\n",
    "      print(\"i = %d Model = %s\" %(i,model_x))\n",
    "      classifier         =   models[i] \n",
    "      smote_df = evaluate_model_performance(model_x, classifier, X_train_SMOTE, y_train_SMOTE, X_test, y_test)\n",
    "      SMOTE_metrics_df =  pd.concat([SMOTE_metrics_df,smote_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aaa026c-8ab9-4ad5-9a3b-29be8a10f5bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "SMOTE_metrics_df.sort_values(by=['Recall Test data'], ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d230d2-fc54-46ab-bd86-8ecbb9b41ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_elapsed = datetime.now() - start_time\n",
    "print('\\nExecution Time for evaluating the performance of 8 models on the data treated for data imbalance')\n",
    "print('Time elapsed (hh:mm:ss.ms) {}'.format(time_elapsed))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a0e9cc-64f7-44af-b243-307b1d231444",
   "metadata": {},
   "source": [
    "### Observations\n",
    "\n",
    "Without applying SMOTE technique to treat the data imbalance, all the models are performing well with the recall values on both training and test datasets are 99% or above.\n",
    "\n",
    "After applying SMOTE technique to treat the data imbalance, all the models are performing well with the recall values on both training and test datasets are 99% or above. Recall values of all the models are 100%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f262c829-586d-4611-81cb-f6b24fc82eb8",
   "metadata": {},
   "source": [
    "### KFold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30384ca9-72a7-4263-9dd0-af6b9298eeec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_performance_KF(name, model, X, y):\n",
    "    from  sklearn.model_selection import  StratifiedKFold\n",
    "    from  sklearn.model_selection import  cross_val_score\n",
    "\n",
    "    # Predict the response for training dataset\n",
    "    classifier   =  model.fit(X, y)\n",
    "    scores_req   =  ['roc_auc', 'precision', 'recall', 'F1 Weighted']\n",
    "    df_model     =  pd.DataFrame()\n",
    "    for scoring in scores_req:\n",
    "        score_fn  =  scoring         \n",
    "\n",
    "        print(\"\\nPerformance Measure : %s\" %scoring)\n",
    "\n",
    "        if scoring == 'F1 Weighted':\n",
    "            score_fn  =  make_scorer (f1_score, average = 'weighted', zero_division = 0)\n",
    "\n",
    "        skf           =   StratifiedKFold(n_splits = 10, shuffle = True, random_state = 12345)\n",
    "        cv_results    =   cross_val_score(model, X, np.ravel(y,order=\"c\"), cv = skf, scoring = score_fn)\n",
    "        msg = \"%s\" % (str(round(cv_results.mean(),2)) + \" ± \" + str(round(cv_results.std(),2)))\n",
    "        print(msg)\n",
    "        df_measures_tuned.at[name, scoring] = round(cv_results.mean(),2)\n",
    "        indices_      =   list(range(1, 11))\n",
    "        df_ind        =   pd.DataFrame({'Score' : scoring, 'Name' : name, 'Resuts' : cv_results}, index = indices_)\n",
    "        df_model      =   pd.concat([df_model, df_ind]) \n",
    "    xdf           = df_model.groupby([\"Name\", \"Score\"]).agg([np.mean, np.std])\n",
    "    return xdf    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fab2aee-053e-4163-b6a4-0995459a67f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Kfold_metrics_df  =  pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a3dcc15-f7eb-4ef5-a458-aa032990f8f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for   i    in   range(len(model_names)):\n",
    "      model_x            =   model_names[i]\n",
    "      classifier         =   models[i] \n",
    "      print(\"i = %d Model = %s\" %(i,model_x))\n",
    "      KFold_df           =   evaluate_model_performance_KF(model_x, classifier, X_, y)\n",
    "      Kfold_metrics_df   =   pd.concat([Kfold_metrics_df, KFold_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e24ffdbd-f8f8-4f68-9d03-5778a475f174",
   "metadata": {},
   "outputs": [],
   "source": [
    "Kfold_metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "012ed8a1-2d7c-437b-b07a-f076ad390274",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "931dbc49-5865-4c66-ad5a-7a07cee2d978",
   "metadata": {},
   "source": [
    "### We need to choose the best model. \n",
    "\n",
    "### Do we need tune the hyper parameters of all the models and again compare?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
